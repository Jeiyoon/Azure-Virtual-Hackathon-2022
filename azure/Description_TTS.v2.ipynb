{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b38965",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 설치하기,  불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cb1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 호출\n",
    "import cv2\n",
    "import os\n",
    "import pafy\n",
    "#!pip install --upgrade youtube_dl (pafy 없다면)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934abf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-vision-computervision in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)\n",
      "Requirement already satisfied: msrest>=0.5.0 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-computervision) (0.6.21)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (0.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2021.10.8)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.26.0)\n",
      "Requirement already satisfied: six in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade azure-cognitiveservices-vision-computervision\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cf74f",
   "metadata": {},
   "source": [
    "# 2. 영상 전처리 (이미지 캡쳐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd243116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영상의 길이 :  148.91543333333334 초\n"
     ]
    }
   ],
   "source": [
    "filepath = 'C:/Users/ansrl/Documents/이문기/11_TAVE/2022 AI_SPARK/비디오요약/tvsum50_ver_1_1/ydata-tvsum50-v1_1/tvsum50_video/_xMr-HKMfVA.mp4'\n",
    "vidcap = cv2.VideoCapture()\n",
    "vidcap.open(filepath)\n",
    "\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS) \n",
    "frame_count = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "duration = frame_count / fps\n",
    "print(\"영상의 길이 : \", duration, \"초\")\n",
    "\n",
    "count = 0                # count 번째 사진\n",
    "increase_width = 10        # 여기서 몇초마다 찍을건지 세팅하면 됌.\n",
    "second = 0\n",
    "success = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93bb630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 초 에서 캡쳐\n",
      "saved frame0.jpg\n",
      "10 초 에서 캡쳐\n",
      "saved frame1.jpg\n",
      "20 초 에서 캡쳐\n",
      "saved frame2.jpg\n",
      "30 초 에서 캡쳐\n",
      "saved frame3.jpg\n",
      "40 초 에서 캡쳐\n",
      "saved frame4.jpg\n",
      "50 초 에서 캡쳐\n",
      "saved frame5.jpg\n",
      "60 초 에서 캡쳐\n",
      "saved frame6.jpg\n",
      "70 초 에서 캡쳐\n",
      "saved frame7.jpg\n",
      "80 초 에서 캡쳐\n",
      "saved frame8.jpg\n",
      "90 초 에서 캡쳐\n",
      "saved frame9.jpg\n",
      "100 초 에서 캡쳐\n",
      "saved frame10.jpg\n",
      "110 초 에서 캡쳐\n",
      "saved frame11.jpg\n",
      "120 초 에서 캡쳐\n",
      "saved frame12.jpg\n",
      "130 초 에서 캡쳐\n",
      "saved frame13.jpg\n",
      "140 초 에서 캡쳐\n",
      "saved frame14.jpg\n",
      "----- Finish Video Capture! -----\n"
     ]
    }
   ],
   "source": [
    "while success and second <= duration:\n",
    "    success,image = vidcap.read()\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC, second * 1000)\n",
    "    print(second, \"초 에서 캡쳐\")\n",
    "    cv2.imwrite(\"../images/frame%d.jpg\" % count, image)\n",
    "    # 왠지 모르겠는데 폴더 중에서 한글이 있으면 작동 안 하는 듯 (캡쳐가 안됨)\n",
    "    print(\"saved frame%d.jpg\" % count)\n",
    "    count += 1   \n",
    "    second += increase_width\n",
    "    if cv2.waitKey(10) == 27:\n",
    "        break\n",
    "        \n",
    "print('----- Finish Video Capture! -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde065ba",
   "metadata": {},
   "source": [
    "# 3. Azure Cognitive Service 이미지분석 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d87f8b",
   "metadata": {},
   "source": [
    "### ※ 참고사항!\n",
    "\n",
    "#### 1) URL 기반으로 Description 만들 때 : Describe_image 메써드 사용\n",
    "- Azure Cognitive Services Computer Vision SDK for Python : 이게 BEST\n",
    "https://docs.microsoft.com/ko-kr/python/api/overview/azure/cognitiveservices-vision-computervision-readme?view=azure-python\n",
    "\n",
    "- <참고만> : 오류뜸 https://docs.microsoft.com/ko-kr/azure/cognitive-services/computer-vision/quickstarts-sdk/image-analysis-client-library?tabs=visual-studio&pivots=programming-language-python\n",
    "\n",
    "- <참고만> : 오류뜸 https://docs.microsoft.com/ko-kr/azure/cognitive-services/computer-vision/vision-api-how-to-topics/howtocallvisionapi?tabs=python\n",
    "\n",
    "\n",
    "#### 2) 로컬 경로에서 Description 만들 때 : describe_image_in_stream 메써드 사용\n",
    "https://github.com/Azure-Samples/cognitive-services-quickstart-code/blob/master/python/ComputerVision/ImageAnalysisQuickstart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d8c5d",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf2d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "from array import array\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a18b6",
   "metadata": {},
   "source": [
    "### 2. 계정 인증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934986ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEND - Authenticate\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Authenticate\n",
    "Authenticates your credentials and creates a client.\n",
    "'''\n",
    "\n",
    "#key와 엔드포인트는 azure portal에서 cognitive service를 만들고 나면 가져올 수 있음\n",
    "#현재의 키와 엔드포인트는 내 계정인데, 다른 컴퓨터에서도 돌아갈지는 모르겠음\n",
    "\n",
    "subscription_key = \"9e14cf501b8b4ca98a27c3bb2391e803\"\n",
    "endpoint = \"https://lmg1111111.cognitiveservices.azure.com/\"\n",
    "\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "'''\n",
    "END - Authenticate\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb3c67",
   "metadata": {},
   "source": [
    "### 3. 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec737d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../images\\\\frame0.jpg', '../images\\\\frame1.jpg', '../images\\\\frame2.jpg', '../images\\\\frame3.jpg', '../images\\\\frame4.jpg', '../images\\\\frame5.jpg', '../images\\\\frame6.jpg', '../images\\\\frame7.jpg', '../images\\\\frame8.jpg', '../images\\\\frame9.jpg', '../images\\\\frame10.jpg', '../images\\\\frame11.jpg', '../images\\\\frame12.jpg', '../images\\\\frame13.jpg', '../images\\\\frame14.jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEND - Quickstart variables\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images used for the examples: Describe an image, Categorize an image, Tag an image, \n",
    "# Detect faces, Detect adult or racy content, Detect the color scheme, \n",
    "# Detect domain-specific content, Detect image types, Detect objects\n",
    "\n",
    "import glob\n",
    "#images_folder = os.path.join (os.path.dirname(os.path.abspath('')), \"images\")\n",
    "image_path = sorted(glob.glob('../images/frame*.jpg'), key=os.path.getctime)\n",
    "print(image_path)\n",
    "# url로 불러오고자 할 때 아래\n",
    "#remote_image_url = \"https://moderatorsampleimages.blob.core.windows.net/samples/sample16.png\"\n",
    "\n",
    "'''\n",
    "END - Quickstart variables\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea214359",
   "metadata": {},
   "source": [
    "### 4. 분석하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ed3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def azure_cv(path):\n",
    "    '''\n",
    "    Describe an Image - local\n",
    "    This example describes the contents of an image with the confidence score.\n",
    "    '''\n",
    "    print(\"===== Describe an Image - local =====\")\n",
    "    # images 폴더에서 image2.png 파일을 불러온다\n",
    "    #혹시나 해서 실험해 봤는데 png든 jpg든 결과에는 관계없다. 확장자에 따라 결과가 달라지지 않는다\n",
    "    local_image_path = os.path.join(path)\n",
    "    local_image = open(local_image_path, \"rb\")\n",
    "    \n",
    "    # Call API\n",
    "    description_result = computervision_client.describe_image_in_stream(local_image)\n",
    "\n",
    "    # Get the captions (descriptions) from the response, with confidence level\n",
    "    print(\"Description of local image: \")\n",
    "    if (len(description_result.captions) == 0):\n",
    "        print(\"No description detected.\")\n",
    "    else:\n",
    "        for caption in description_result.captions:\n",
    "            print(\"'{}' : '{}' with confidence {:.2f}%\".format(path, caption.text, caption.confidence * 100))\n",
    "    print()\n",
    "    return caption.text\n",
    "    #text만 따로 뽑아쓰려면 caption[0].text로 뽑아쓸 수 있다\n",
    "    #print(description_result.captions[0].text)\n",
    "    '''\n",
    "    END - Describe an Image - local\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb015c",
   "metadata": {},
   "source": [
    "### 5. 분석하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfff75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame0.jpg' : 'a cube with a black background' with confidence 27.32%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame1.jpg' : 'a cube with a black background' with confidence 27.32%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame2.jpg' : 'a close-up of a card' with confidence 42.50%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame3.jpg' : 'a sign with text' with confidence 25.38%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame4.jpg' : 'a group of people in a room' with confidence 48.67%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame5.jpg' : 'a group of people sitting in a room' with confidence 51.26%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame6.jpg' : 'a person playing a saxophone' with confidence 57.50%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame7.jpg' : 'a person holding a baby' with confidence 31.94%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame8.jpg' : 'a group of people playing instruments' with confidence 49.87%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame9.jpg' : 'a person with a mask' with confidence 25.95%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame10.jpg' : 'a close up of a piece of food' with confidence 31.11%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame11.jpg' : 'a group of people playing instruments' with confidence 59.61%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame12.jpg' : 'a group of people posing for the camera' with confidence 55.17%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame13.jpg' : 'a person playing a saxophone' with confidence 50.71%\n",
      "\n",
      "===== Describe an Image - local =====\n",
      "Description of local image: \n",
      "'../images\\frame14.jpg' : 'a woman looking at a man' with confidence 42.04%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z = []\n",
    "for path in image_path:\n",
    "    feature = [azure_cv(path)]\n",
    "    for ele in feature:\n",
    "        Z.append(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b12c8",
   "metadata": {},
   "source": [
    "### 6. csv 파일로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5184b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_Z = pd.DataFrame(Z)\n",
    "df_Z.to_csv('../images/azure_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ecff3",
   "metadata": {},
   "source": [
    "# 4. TTS 가즈아"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208d05b",
   "metadata": {},
   "source": [
    "### ※ 참고사항!\n",
    "\n",
    "#### 1. 텍스트 음성 변환\n",
    "https://docs.microsoft.com/ko-kr/azure/cognitive-services/speech-service/get-started-text-to-speech?tabs=terminal&pivots=programming-language-python\n",
    "\n",
    "#### 2. 텍스트 파일 저장\n",
    "https://docs.microsoft.com/ko-kr/azure/cognitive-services/speech-service/how-to-speech-synthesis?tabs=script%2Cbrowserjs%2Cterminal&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949dd940",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13fa94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-speech in c:\\users\\ansrl\\anaconda3\\lib\\site-packages (1.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcce50d",
   "metadata": {},
   "source": [
    "### 2. 인증부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2019c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "\n",
    "# 여기서도 key와 region은 portal에서 cognitive service를 만들면 뽑을 수 있다\n",
    "speech_config = speechsdk.SpeechConfig(subscription=\"4273fb5b821a4785bc85836bea53d2a1\", region=\"KoreaCentral\")\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# 제니씨의 음성으로 들어보자\n",
    "speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98d7a4",
   "metadata": {},
   "source": [
    "### 3. 스피커 출력 + 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f88e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a cube with a black background]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a cube with a black background]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a close-up of a card]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a sign with text]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a group of people in a room]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a group of people sitting in a room]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a person playing a saxophone]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a person holding a baby]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a group of people playing instruments]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a person with a mask]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a close up of a piece of food]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a group of people playing instruments]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a group of people posing for the camera]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a person playing a saxophone]\n",
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [a woman looking at a man]\n"
     ]
    }
   ],
   "source": [
    "for a in range(0, len(Z)):\n",
    "    # Get text from the console and synthesize to the default speaker.\n",
    "    print(\"Enter some text that you want to speak >\")\n",
    "\n",
    "    #text = input()을 넣으면 따로 내가 출력하고 싶은 글귀를 넣을 수도 있다\n",
    "    #일단은 아까의 분석결과를 넣어보도록 하자\n",
    "    text = Z[a]\n",
    "    \n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    #스피커로 출력하기\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(filename='../images/%d.wav' % a)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    synthesizer.speak_text_async(Z[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab19259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
